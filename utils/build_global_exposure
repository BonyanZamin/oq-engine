#!/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
# Copyright (C) 2023, GEM Foundation
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import logging
import numpy
import h5py
from openquake.baselib import general, hdf5, sap, performance
from openquake.baselib.parallel import Starmap
from openquake.hazardlib.geo.utils import geohash3
from openquake.commonlib.datastore import build_dstore_log
from openquake.risklib.asset import _get_exposure

U16 = numpy.uint16
U32 = numpy.uint32
F32 = numpy.float32
CONV = {n: F32 for n in '''
BUILDINGS COST_CONTENTS_USD COST_NONSTRUCTURAL_USD COST_PER_AREA_USD
COST_STRUCTURAL_USD LATITUDE LONGITUDE OCCUPANTS_PER_ASSET
OCCUPANTS_PER_ASSET_AVERAGE OCCUPANTS_PER_ASSET_DAY
OCCUPANTS_PER_ASSET_NIGHT OCCUPANTS_PER_ASSET_TRANSIT
TOTAL_AREA_SQM TOTAL_REPL_COST_USD'''.split()}
CONV['ASSET_ID'] = (numpy.string_, 24)
CONV[None] = str
TAGS = {'TAXONOMY': [], 'ID_0': [], 'ID_1': [], 'OCCUPANCY': []}
IGNORE = set('NAME_0 NAME_1 SETTLEMENT'.split())


def add_geohash3(array):
    if len(array) == 0:
        return ()
    dt = array.dtype
    dtlist = [('geohash3', U16)] + [(n, dt[n]) for n in dt.names]
    out = numpy.zeros(len(array), dtlist)
    for n in dt.names:
        out[n] = array[n]
        out['geohash3'] = geohash3(array['LONGITUDE'], array['LATITUDE'])
    return out


def fix(arr):
    # prepend the country to ASSET_ID and ID_1
    ID0 = arr['ID_0']
    ID1 = arr['ID_1']
    arr['ASSET_ID'] = numpy.char.add(numpy.array(ID0, 'S3'), arr['ASSET_ID'])
    for i, (id0, id1) in enumerate(zip(ID0, ID1)):
        if not id1.startswith(id0):
            ID1[i] = '%s-%s' % (id0, ID1[i])


def array_by_geohash(arrays, monitor):
    for array in arrays:
        arr = add_geohash3(array)
        fix(arr)
        for gh in numpy.unique(arr['geohash3']):
            yield gh, arr[arr['geohash3']==gh]


def exposure_by_geohash(exposure_xml, common, monitor):
    exposure, _ = _get_exposure(exposure_xml)
    small = []
    for fname in exposure.datafiles:
        aw = hdf5.read_csv(fname, CONV, errors='ignore', usecols=common)
        if hasattr(aw, 'array') and len(aw.array):
            for i, slc in enumerate(
                    general.gen_slices(0, len(aw.array), 2_000_000)):
                arr = aw.array[slc]
                if i == 0:  # smaller than 500k elements
                    small.append(arr)
                else:
                    print(fname)
                    yield array_by_geohash, [arr]
    yield from array_by_geohash(small, monitor)


def collect_exposures(grm_dir):
    """
    Collect the files of kind Exposure_<Country>.xml
    """
    out = []
    for region in os.listdir(grm_dir):
        expodir = os.path.join(grm_dir, region, 'Exposure', 'Exposure')
        if not os.path.exists(expodir):
            continue
        for fname in os.listdir(expodir):
            if fname.startswith('Exposure_'):  # i.e. Exposure_Chile.xml
                out.append(os.path.join(expodir, fname))
    return out


def read_world_exposure(grm_dir, dstore):
    """
    Read the exposure files for the entire world (assume some conventions
    on the file names are respected).

    :param grm_dir: directory containing the global risk model
    """
    fnames = collect_exposures(grm_dir)
    csvfiles = []
    for fname in fnames:
        exposure, _ = _get_exposure(fname)
        csvfiles.extend(exposure.datafiles)

    common = sorted(hdf5.read_common_header(csvfiles) - IGNORE)
    assert common, 'There is no common header subset among %s' % csvfiles

    dtlist = [(t, U32) for t in TAGS] + \
        [(f, F32) for f in set(CONV)-set(TAGS)-{'ASSET_ID', None}] + \
        [('ASSET_ID', h5py.string_dtype('ascii', 25))]
    dstore.create_df('exposure', dtlist, 'gzip')
    for tagname in TAGS:
        dstore.create_dset('tagcol/' + tagname, U32)
    slc_dt = numpy.dtype([('gh3', U16), ('start', U32), ('stop', U32)])
    dstore.create_dset('exposure/slice_by_gh3', slc_dt, fillvalue=None)

    dstore.swmr_on()
    smap = Starmap(exposure_by_geohash, [(f, common) for f in fnames],
                   h5=dstore.hdf5)
    s = 0
    for gh3, arr in smap:
        for name in common:
            if name in TAGS:
                TAGS[name].append(arr[name])
            else:
                hdf5.extend(dstore['exposure/' + name], arr[name])
        n = len(arr)
        slc = numpy.array([(gh3, s, s + n)], slc_dt)
        hdf5.extend(dstore['exposure/slice_by_gh3'], slc)
        s += n
    Starmap.shutdown()

    for tagname in TAGS:
        tagvalues = numpy.concatenate(TAGS[tagname])
        uvals, inv = numpy.unique(tagvalues, return_inverse=1)
        logging.info('Storing %s[%d]', tagname, len(uvals))
        hdf5.extend(dstore[f'exposure/{tagname}'], inv)
        dstore['tagcol/' + tagname] = uvals
    logging.info('Total {:_d} assets'.format(len(inv)))


def main(grm_dir):
    """
    Storing global exposure
    """
    mon = performance.Monitor(measuremem=True)
    dstore, log = build_dstore_log()
    with dstore, log:
        with mon:
            read_world_exposure(grm_dir, dstore)
        logging.info(mon)
main.grm_dir = 'global risk model directory'


if __name__ == '__main__':
    sap.run(main)
