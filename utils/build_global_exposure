#!/bin/env python
# -*- coding: utf-8 -*-
# vim: tabstop=4 shiftwidth=4 softtabstop=4
#
# Copyright (C) 2023, GEM Foundation
#
# OpenQuake is free software: you can redistribute it and/or modify it
# under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# OpenQuake is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with OpenQuake.  If not, see <http://www.gnu.org/licenses/>.

import os
import io
import zlib
import logging
import collections
import pandas
import numpy
import h5py
from openquake.baselib import general, hdf5, sap, performance
from openquake.baselib.parallel import Starmap
from openquake.hazardlib.geo.utils import geohash3
from openquake.commonlib.datastore import build_dstore_log
from openquake.risklib.asset import _get_exposure

U16 = numpy.uint16
U32 = numpy.uint32
F32 = numpy.float32
CONV = {n: F32 for n in '''
BUILDINGS COST_CONTENTS_USD COST_NONSTRUCTURAL_USD COST_PER_AREA_USD
COST_STRUCTURAL_USD LATITUDE LONGITUDE OCCUPANTS_PER_ASSET
OCCUPANTS_PER_ASSET_AVERAGE OCCUPANTS_PER_ASSET_DAY
OCCUPANTS_PER_ASSET_NIGHT OCCUPANTS_PER_ASSET_TRANSIT
TOTAL_AREA_SQM TOTAL_REPL_COST_USD'''.split()}
CONV['ASSET_ID'] = (numpy.string_, 24)
for f in (None, 'ID_1'):
    CONV[f] = str
TAGS = {'TAXONOMY': [], 'ID_0': [], 'ID_1': [], 'OCCUPANCY': []}
IGNORE = set('NAME_0 NAME_1 SETTLEMENT'.split())
File = collections.namedtuple('File', 'fname weight fields')


def add_geohash3(array):
    if len(array) == 0:
        return ()
    dt = array.dtype
    dtlist = [('geohash3', U16)] + [(n, dt[n]) for n in dt.names]
    out = numpy.zeros(len(array), dtlist)
    for n in dt.names:
        out[n] = array[n]
        out['geohash3'] = geohash3(array['LONGITUDE'], array['LATITUDE'])
    return out


def fix(arr):
    # prepend the country to ASSET_ID and ID_1
    ID0 = arr['ID_0']
    ID1 = arr['ID_1']
    arr['ASSET_ID'] = numpy.char.add(numpy.array(ID0, 'S3'), arr['ASSET_ID'])
    for i, (id0, id1) in enumerate(zip(ID0, ID1)):
        if not id1.startswith(id0):
            ID1[i] = '%s-%s' % (id0, ID1[i])


def collect_exposures(grm_dir):
    """
    Collect the files of kind Exposure_<Country>.xml.

    :returns: xmlfiles, csvfiles, csvsizes
    """
    out = []
    sizes = []
    csvfiles = []
    for region in os.listdir(grm_dir):
        expodir = os.path.join(grm_dir, region, 'Exposure', 'Exposure')
        if not os.path.exists(expodir):
            continue
        for fname in os.listdir(expodir):
            if fname.startswith('Exposure_'):  # i.e. Exposure_Chile.xml
                fullname = os.path.join(expodir, fname)
                out.append(fullname)
                exposure, _ = _get_exposure(fullname)
                csvfiles.extend(exposure.datafiles)
                sizes.extend(map(os.path.getsize, exposure.datafiles))
    return out, csvfiles, sizes


def exposure_by_geohash(lines, names, common, monitor):
    if isinstance(lines, bytes):
        data = io.BytesIO(zlib.decompress(lines))
    else:
        data = io.StringIO(lines)
    df = pandas.read_csv(data, names=names, dtype=CONV, usecols=common)
    dt = hdf5.build_dt(CONV, names, '<BytesIO>')
    array = numpy.zeros(len(df), dt)
    for col in df.columns:
        array[col] = df[col].to_numpy()
    array = add_geohash3(array)
    fix(array)
    for gh in numpy.unique(array['geohash3']):
        yield gh, array[array['geohash3']==gh]


def gen_tasks(rows, monitor):
    for row in rows:
        f = open(row.fname, newline='', encoding='utf-8-sig', errors='ignore')
        with f:
            lines = list(f)
        header = [col.strip() for col in lines[0].split(',')]
        for i, block in enumerate(general.block_splitter(lines[1:], 200_000)):
            data = '\r\n'.join(block)
            if i == 0:
                yield from exposure_by_geohash(
                    data, header, row.fields, monitor)
            else:
                print(row.fname)
                data = zlib.compress(data.encode('utf8'))
                yield exposure_by_geohash, data, header, row.fields


def read_world_exposure(grm_dir, dstore):
    """
    Read the exposure files for the entire world (assume some conventions
    on the file names are respected).

    :param grm_dir: directory containing the global risk model
    """
    fnames, csvfiles, csvsizes = collect_exposures(grm_dir)
    common = sorted(hdf5.read_common_header(csvfiles) - IGNORE)
    assert common, 'There is no common header subset among %s' % csvfiles

    dtlist = [(t, U32) for t in TAGS] + \
        [(f, F32) for f in set(CONV)-set(TAGS)-{'ASSET_ID', None}] + \
        [('ASSET_ID', h5py.string_dtype('ascii', 25))]
    dstore.create_df('exposure', dtlist, 'gzip')
    for tagname in TAGS:
        dstore.create_dset('tagcol/' + tagname, U32)
    slc_dt = numpy.dtype([('gh3', U16), ('start', U32), ('stop', U32)])
    dstore.create_dset('exposure/slice_by_gh3', slc_dt, fillvalue=None)

    dstore.swmr_on()
    files = [File(c, w, common) for c, w in zip(csvfiles, csvsizes)]
    smap = Starmap.apply(gen_tasks, (files,), h5=dstore.hdf5)
    s = 0
    for gh3, arr in smap:
        for name in common:
            if name in TAGS:
                TAGS[name].append(arr[name])
            else:
                hdf5.extend(dstore['exposure/' + name], arr[name])
        n = len(arr)
        slc = numpy.array([(gh3, s, s + n)], slc_dt)
        hdf5.extend(dstore['exposure/slice_by_gh3'], slc)
        s += n
    Starmap.shutdown()

    for tagname in TAGS:
        tagvalues = numpy.concatenate(TAGS[tagname])
        uvals, inv = numpy.unique(tagvalues, return_inverse=1)
        logging.info('Storing %s[%d]', tagname, len(uvals))
        hdf5.extend(dstore[f'exposure/{tagname}'], inv)
        dstore['tagcol/' + tagname] = uvals


def main(grm_dir):
    """
    Storing global exposure
    """
    mon = performance.Monitor(measuremem=True)
    dstore, log = build_dstore_log()
    with dstore, log:
        with mon:
            read_world_exposure(grm_dir, dstore)
        logging.info(mon)
main.grm_dir = 'global risk model directory'


if __name__ == '__main__':
    sap.run(main)
