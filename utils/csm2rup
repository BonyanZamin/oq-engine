#!/usr/bin/env python
import os
import logging
import operator
import numpy
import pandas
from openquake.baselib import sap, parallel, general, hdf5
from openquake.commonlib import readinput, datastore, logs

MAX_RUPS_PER_SOURCE = 1_000_000_000
rupfields = dict(
    mag=numpy.float32,
    occurrence_rate=numpy.float32,
    hypo_lon=numpy.float32,
    hypo_lat=numpy.float32,
    hypo_dep=numpy.float32,
    grp_id=numpy.uint16,
    rup_id=numpy.uint64,
    # add more if you like
)


def build_rupdict(srcs):
    """
    :param srcs: a list of sources of the same source group
    """
    dic = {f: [] for f in rupfields}
    for src in srcs:
        for rupno, rup in enumerate(src.iter_ruptures()):
            assert rupno < MAX_RUPS_PER_SOURCE
            dic['mag'].append(rup.mag)
            dic['occurrence_rate'].append(rup.occurrence_rate)
            dic['hypo_lon'].append(rup.hypocenter.x)
            dic['hypo_lat'].append(rup.hypocenter.y)
            dic['hypo_dep'].append(rup.hypocenter.z)
            dic['grp_id'].append(src.grp_id)
            dic['rup_id'].append(src.id * MAX_RUPS_PER_SOURCE + rupno)
    return {k: rupfields[k](v) for k, v in dic.items()}



def csm2rup(csm, dstore):
    """
    :param csm: CompositeSourceModel instance
    :param dstore: DataStore where to save the ruptures
    """
    def src_weight(src):
        # make point sources 100x lighter
        return .01 if src.code == b'P' else 1

    sources = csm.get_sources()
    dstore.create_df('rup', rupfields.items(), compression='gzip')  # 10x gain!
    for rupdict in parallel.Starmap.apply(
            build_rupdict, (sources,), weight=src_weight,
            key=operator.attrgetter('grp_id'), h5=dstore,
            concurrent_tasks=1000):
        for k, v in rupdict.items():
            hdf5.extend(dstore[f'rup/{k}'], v)
    nrups = len(dstore['rup/mag'])
    size = general.humansize(os.path.getsize(dstore.filename))
    logging.info('Stored {:_d} ruptures [{}] inside {}'.format(
        nrups, size, dstore.filename))
 

def main(job_ini):
    """
    Convert a composite source model into a DataFrame of ruptures
    """
    # first of all, initialize logs
    job_id = datastore.init("job")
    oq = readinput.get_oqparam(job_ini)
    dstore = datastore.new(job_id, oq)
    try:
        csm = readinput.get_composite_source_model(oq)
        csm2rup(csm, dstore)
    except Exception:
        datastore.dbcmd('finish', job_id, 'failed')
        raise
    else:
        datastore.dbcmd('finish', job_id, 'complete')
    finally:
        dstore.close()
        parallel.Starmap.shutdown()


if __name__ == '__main__':
    sap.run(main)
