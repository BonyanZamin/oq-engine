Rupture sampling: how does it work?
===================================

In this section we will explain how the sampling of ruptures in event based
calculations works, at least for the case of poissonian sources.
As an example, we will consider the following point source:

>>> from openquake.hazardlib import nrml
>>> src = nrml.get('''\
... <pointSource id="1" name="Point Source"
...              tectonicRegion="Active Shallow Crust">
...     <pointGeometry>
...         <gml:Point><gml:pos>179.5 0</gml:pos></gml:Point>
...         <upperSeismoDepth>0</upperSeismoDepth>
...         <lowerSeismoDepth>10</lowerSeismoDepth>
...     </pointGeometry>
...     <magScaleRel>WC1994</magScaleRel>
...     <ruptAspectRatio>1.5</ruptAspectRatio>
...     <truncGutenbergRichterMFD aValue="3" bValue="1" minMag="5" maxMag="7"/>
...     <nodalPlaneDist>
...         <nodalPlane dip="30" probability="1" strike="45" rake="90" />
...     </nodalPlaneDist>
...     <hypoDepthDist>
...         <hypoDepth depth="4" probability="1"/>
...     </hypoDepthDist>
... </pointSource>''', investigation_time=1, width_of_mfd_bin=1.0)

The source here is particularly simple. It contains only two ruptures,
because with a `width_of_mfd_bin of 1 there only two magnitudes in the
range from 5 to 7:

>>> [(mag1, rate1), (mag2, rate2)] = src.get_annual_occurrence_rates()
>>> mag1
5.5
>>> mag2
6.5

The occurrence rates are respectively 0.009 and 0.0009. So, if we set
the number of stochastic event sets to 1,000,000

>>> num_ses = 1_000_000

we would expect the first rupture (the one with magnitude 5.5) to
occur around 9000 times and the second rupture (the one with magnitude
6.5) to occur around 900 times. Clearly the exact numbers will depend on
the stochastic seed; if we set

>>> import numpy.random as r
>>> r.seed(42)

then we will have

>>> r.poisson(rate1 * num_ses)
8966
>>> r.poisson(rate2 * num_ses)
921

These are the number of occurrences of each rupture in the effective
investigation time, i.e. the investigation time multiplied by the
number of stochastic event sets.

The total number of events generated by the source will be
8966 + 921 = 9887, with ~91% of the events associated to the first
rupture and ~9% of the events associated to the second rupture.

Since the details of the seed algorithm changes at each release of
the engine, if you run an event based calculation with the same
parameters you will not get exactly the same number of events,
but something very close. After running the calculation inside
the datastore, in the ``ruptures`` dataset you will find the two
ruptures, their occurrence rates and their integer number of
occurrences (``n_occ`). If the effective investigation time is large
enough the relation

  ``n_occ ~ occurrence_rate * investigation_time * num_ses``

will hold. If the effective investigation time is not large enough, or the
occurrence rate is extremely small, then there will be big differences
between the expected number of occurrences and ``n_occ``, as well as a
strong seed dependency.

Users wanting to know the nitty-gritty details should look at the code,
inside hazardlib/source/base.py, to the method
``src.sample_ruptures(eff_num_ses, ses_seed)``.
